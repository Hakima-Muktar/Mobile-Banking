{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#data path\n",
    "import pandas as pd\n",
    "# Import numpy for numerical operations (though mostly used implicitly by pandas here)\n",
    "import numpy as np\n",
    "# Import datetime class from datetime module to handle date and time objects\n",
    "from datetime import datetime\n",
    "# Import re module for regular expression operations (used for text cleaning)\n",
    "import re\n",
    "# Import DATA_PATHS dictionary from the local config module\n",
    "project_root = r\"C:\\Users\\user\\Desktop\\Project\\Mobile-Banking\\src\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "# Now we can safely import config\n",
    "from config import DATA_PATHS\n",
    "from src.config import DATA_PATHS\n",
    "\n",
    "class ReviewPreprocessor:\n",
    "    def __init__(self, input_path=None, output_path=None):\n",
    "\n",
    "        # Set the input path: use the provided argument, or default to DATA_PATHS['raw_reviews'] from config\n",
    "        self.input_path = input_path or DATA_PATHS['raw_reviews']\n",
    "        # Set the output path: use the provided argument, or default to DATA_PATHS['processed_reviews'] from config\n",
    "        self.output_path = output_path or DATA_PATHS['processed_reviews']\n",
    "        # Initialize an empty DataFrame attribute to hold our data\n",
    "        self.df = None\n",
    "        # Initialize a dictionary to keep track of processing statistics (counts, errors, etc.)\n",
    "        self.stats = {}\n",
    "def load_data(self):\n",
    "    print(\"Loading data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load raw reviews data\"\"\"\n",
    "        # Print a message indicating that data loading has started\n",
    "        #  print(\"Loading raw data...\")\n",
    "try:\n",
    "            # Read the CSV file at self.input_path into a pandas DataFrame\n",
    "            self.df = pd.read_csv(self.input_path)\n",
    "            # Print the number of records loaded\n",
    "            print(f\"Loaded {len(self.df)} reviews\")\n",
    "            # Record the initial number of records in our stats dictionary\n",
    "            self.stats['original_count'] = len(self.df)\n",
    "            # Return True to indicate success\n",
    "return True\n",
    "except FileNotFoundError:\n",
    "            # Handle the specific error where the file does not exist\n",
    "            print(f\"ERROR: File not found: {self.input_path}\")\n",
    "            # Return False to indicate failure\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            # Handle any other general errors that occur during loading\n",
    "            print(f\"ERROR: Failed to load data: {str(e)}\")\n",
    "            # Return False to indicate failure\n",
    "            return False\n",
    "\n",
    "    def check_missing_data(self):\n",
    "        \"\"\"Check for missing data\"\"\"\n",
    "        # Print a header for this step [1/6]\n",
    "        print(\"\\n[1/6] Checking for missing data...\")\n",
    "\n",
    "        # Calculate the count of missing (null) values for each column\n",
    "        missing = self.df.isnull().sum()\n",
    "        # Calculate the percentage of missing values for each column\n",
    "        missing_pct = (missing / len(self.df)) * 100\n",
    "\n",
    "        # Print the section header\n",
    "        print(\"\\nMissing values:\")\n",
    "        # Loop through each column name in the index of the 'missing' series\n",
    "        for col in missing.index:\n",
    "            # If the column has at least one missing value\n",
    "            if missing[col] > 0:\n",
    "                # Print the column name, count of missing values, and percentage\n",
    "                print(f\"  {col}: {missing[col]} ({missing_pct[col]:.2f}%)\")\n",
    "\n",
    "        # Store the dictionary of missing counts in our stats for reporting later\n",
    "        self.stats['missing_before'] = missing.to_dict()\n",
    "\n",
    "        # Define a list of columns that are absolutely required for our analysis\n",
    "        critical_cols = ['review_text', 'rating', 'bank_name']\n",
    "        # Calculate missing values just for these critical columns\n",
    "        missing_critical = self.df[critical_cols].isnull().sum()\n",
    "\n",
    "        # If there are any missing values in critical columns\n",
    "        if missing_critical.sum() > 0:\n",
    "            # Print a warning message\n",
    "            print(\"\\nWARNING: Missing values in critical columns:\")\n",
    "            # Print the counts of missing values for the critical columns that have them\n",
    "            print(missing_critical[missing_critical > 0])\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing values\"\"\"\n",
    "        # Print a header for this step [2/6]\n",
    "        print(\"\\n[2/6] Handling missing values...\")\n",
    "\n",
    "        # Define the critical columns again\n",
    "        critical_cols = ['review_text', 'rating', 'bank_name']\n",
    "        # Store the count before dropping rows\n",
    "        before_count = len(self.df)\n",
    "        # Drop any rows that have missing values (NaN) in the critical columns\n",
    "        self.df = self.df.dropna(subset=critical_cols)\n",
    "        # Calculate how many rows were removed\n",
    "        removed = before_count - len(self.df)\n",
    "\n",
    "        # If any rows were removed, print a message\n",
    "        if removed > 0:\n",
    "            print(f\"Removed {removed} rows with missing critical values\")\n",
    "\n",
    "        # For the 'user_name' column, fill missing values with the string 'Anonymous'\n",
    "        self.df['user_name'] = self.df['user_name'].fillna('Anonymous')\n",
    "        # For the 'thumbs_up' column, fill missing values with 0\n",
    "        self.df['thumbs_up'] = self.df['thumbs_up'].fillna(0)\n",
    "        # For the 'reply_content' column, fill missing values with an empty string\n",
    "        self.df['reply_content'] = self.df['reply_content'].fillna('')\n",
    "\n",
    "        # Record the number of rows removed due to missing critical data\n",
    "        self.stats['rows_removed_missing'] = removed\n",
    "        # Record the new total count in stats\n",
    "        self.stats['count_after_missing'] = len(self.df)\n",
    "\n",
    "    def normalize_dates(self):\n",
    "        \"\"\"Normalize date formats to YYYY-MM-DD\"\"\"\n",
    "        # Print a header for this step [3/6]\n",
    "        print(\"\\n[3/6] Normalizing dates...\")\n",
    "\n",
    "        try:\n",
    "            # Convert the 'review_date' column to pandas datetime objects\n",
    "            # This handles various string formats automatically\n",
    "            self.df['review_date'] = pd.to_datetime(self.df['review_date'])\n",
    "\n",
    "            # Convert the datetime objects to just date objects (YYYY-MM-DD), removing time info\n",
    "            self.df['review_date'] = self.df['review_date'].dt.date\n",
    "\n",
    "            # Extract the year from the date and create a new 'review_year' column\n",
    "            self.df['review_year'] = pd.to_datetime(self.df['review_date']).dt.year\n",
    "            # Extract the month from the date and create a new 'review_month' column\n",
    "            self.df['review_month'] = pd.to_datetime(self.df['review_date']).dt.month\n",
    "\n",
    "            # Print the range of dates found in the data (minimum and maximum)\n",
    "            print(f\"Date range: {self.df['review_date'].min()} to {self.df['review_date'].max()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle errors if date conversion fails\n",
    "            print(f\"WARNING: Error normalizing dates: {str(e)}\")\n",
    "\n",
    "    def clean_text(self):\n",
    "        \"\"\"Clean review text\"\"\"\n",
    "        # Print a header for this step [4/6]\n",
    "        print(\"\\n[4/6] Cleaning text...\")\n",
    "\n",
    "        def clean_review_text(text):\n",
    "            \"\"\"Inner function to clean individual review text strings\"\"\"\n",
    "            # If the text is NaN (missing) or empty string, return empty string\n",
    "            if pd.isna(text) or text == '':\n",
    "                return ''\n",
    "\n",
    "            # Convert the input to a string type (safety check)\n",
    "            text = str(text)\n",
    "\n",
    "            # Use regex to replace multiple whitespace characters (spaces, tabs, newlines) with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # Remove leading and trailing whitespace from the string\n",
    "            text = text.strip()\n",
    "\n",
    "            # Return the cleaned text\n",
    "            return text\n",
    "\n",
    "        # Apply the 'clean_review_text' function to every element in the 'review_text' column\n",
    "        self.df['review_text'] = self.df['review_text'].apply(clean_review_text)\n",
    "\n",
    "        # Store the count before removing empty reviews\n",
    "        before_count = len(self.df)\n",
    "        # Keep only rows where the length of 'review_text' is greater than 0\n",
    "        self.df = self.df[self.df['review_text'].str.len() > 0]\n",
    "        # Calculate how many empty reviews were removed\n",
    "        removed = before_count - len(self.df)\n",
    "\n",
    "        # If rows were removed, print a message\n",
    "        if removed > 0:\n",
    "            print(f\"Removed {removed} reviews with empty text\")\n",
    "\n",
    "        # Create a new column 'text_length' containing the character count of the review text\n",
    "        self.df['text_length'] = self.df['review_text'].str.len()\n",
    "\n",
    "        # Record statistics about text cleaning\n",
    "        self.stats['empty_reviews_removed'] = removed\n",
    "        self.stats['count_after_cleaning'] = len(self.df)\n",
    "\n",
    "    def validate_ratings(self):\n",
    "        \"\"\"Validate rating values (should be 1-5)\"\"\"\n",
    "        # Print a header for this step [5/6]\n",
    "        print(\"\\n[5/6] Validating ratings...\")\n",
    "\n",
    "        # Find rows where 'rating' is less than 1 OR greater than 5\n",
    "        invalid = self.df[(self.df['rating'] < 1) | (self.df['rating'] > 5)]\n",
    "\n",
    "        # If there are any invalid ratings\n",
    "        if len(invalid) > 0:\n",
    "            # Print a warning with the count of invalid ratings\n",
    "            print(f\"WARNING: Found {len(invalid)} reviews with invalid ratings\")\n",
    "            # Filter the DataFrame to keep only rows where rating is between 1 and 5 (inclusive)\n",
    "            self.df = self.df[(self.df['rating'] >= 1) & (self.df['rating'] <= 5)]\n",
    "        else:\n",
    "            # If all ratings are valid, print a confirmation\n",
    "            print(\"All ratings are valid (1-5)\")\n",
    "\n",
    "        # Record the number of invalid ratings removed\n",
    "        self.stats['invalid_ratings_removed'] = len(invalid)\n",
    "\n",
    "    def prepare_final_output(self):\n",
    "        \"\"\"Prepare final output format\"\"\"\n",
    "        # Print a header for this step [6/6]\n",
    "        print(\"\\n[6/6] Preparing final output...\")\n",
    "\n",
    "        # Define a list of columns in the desired order for the final output file\n",
    "        output_columns = [\n",
    "            'review_id',\n",
    "            'review_text',\n",
    "            'rating',\n",
    "            'review_date',\n",
    "            'review_year',\n",
    "            'review_month',\n",
    "            'bank_code',\n",
    "            'bank_name',\n",
    "            'user_name',\n",
    "            'thumbs_up',\n",
    "            'text_length',\n",
    "            'source'\n",
    "        ]\n",
    "\n",
    "        # Filter the list to include only columns that actually exist in our DataFrame\n",
    "        # This prevents errors if a column was missed in previous steps\n",
    "        output_columns = [col for col in output_columns if col in self.df.columns]\n",
    "        # Reorder the DataFrame columns according to our list\n",
    "        self.df = self.df[output_columns]\n",
    "\n",
    "        # Sort the DataFrame first by 'bank_code' (ascending) and then by 'review_date' (descending/newest first)\n",
    "        self.df = self.df.sort_values(['bank_code', 'review_date'], ascending=[True, False])\n",
    "\n",
    "        # Reset the index of the DataFrame so it starts from 0 to N-1 cleanly\n",
    "        # drop=True prevents the old index from being added as a new column\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "        # Print the final count of reviews\n",
    "        print(f\"Final dataset: {len(self.df)} reviews\")\n",
    "\n",
    "    def save_data(self):\n",
    "        \"\"\"Save processed data\"\"\"\n",
    "        # Print a message indicating saving has started\n",
    "        print(\"\\nSaving processed data...\")\n",
    "\n",
    "        try:\n",
    "            # Create the directory for the output file if it doesn't already exist\n",
    "            # os.path.dirname gets the folder part of the file path\n",
    "            os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n",
    "\n",
    "            # Write the DataFrame to a CSV file at self.output_path\n",
    "            # index=False prevents writing the row numbers (0, 1, 2...) to the file\n",
    "            self.df.to_csv(self.output_path, index=False)\n",
    "            # Print a confirmation message with the path\n",
    "            print(f\"Data saved to: {self.output_path}\")\n",
    "\n",
    "            # Record the final count in stats\n",
    "            self.stats['final_count'] = len(self.df)\n",
    "            # Return True to indicate success\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any errors during saving\n",
    "            print(f\"ERROR: Failed to save data: {str(e)}\")\n",
    "            # Return False to indicate failure\n",
    "            return False\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate preprocessing report\"\"\"\n",
    "        # Print a separator line\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        # Print the report title\n",
    "        print(\"PREPROCESSING REPORT\")\n",
    "        # Print a separator line\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Print various statistics gathered during the process using .get() to avoid errors if key is missing\n",
    "        print(f\"\\nOriginal records: {self.stats.get('original_count', 0)}\")\n",
    "        print(f\"Records with missing critical data: {self.stats.get('rows_removed_missing', 0)}\")\n",
    "        print(f\"Empty reviews removed: {self.stats.get('empty_reviews_removed', 0)}\")\n",
    "        print(f\"Invalid ratings removed: {self.stats.get('invalid_ratings_removed', 0)}\")\n",
    "        print(f\"Final records: {self.stats.get('final_count', 0)}\")\n",
    "\n",
    "        # Calculate data quality percentage metrics\n",
    "        if self.stats.get('original_count', 0) > 0:\n",
    "            # Retention rate = (Final / Original) * 100\n",
    "            # We use .get(..., 1) for denominator to avoid division by zero if original_count is missing\n",
    "            retention_rate = (self.stats.get('final_count', 0) / self.stats.get('original_count', 1)) * 100\n",
    "            # Error rate is the inverse of retention rate\n",
    "            error_rate = 100 - retention_rate\n",
    "            print(f\"\\nData retention rate: {retention_rate:.2f}%\")\n",
    "            print(f\"Data error rate: {error_rate:.2f}%\")\n",
    "\n",
    "            # Assess quality based on error rate thresholds\n",
    "            if error_rate < 5:\n",
    "                print(\"‚úì Data quality: EXCELLENT (<5% errors)\")\n",
    "            elif error_rate < 10:\n",
    "                print(\"‚úì Data quality: GOOD (<10% errors)\")\n",
    "            else:\n",
    "                print(\"‚ö† Data quality: NEEDS ATTENTION (>10% errors)\")\n",
    "\n",
    "        # Print statistics about the reviews per bank\n",
    "        if self.df is not None:\n",
    "            print(\"\\nReviews per bank:\")\n",
    "            # Count occurrences of each unique value in 'bank_name'\n",
    "            bank_counts = self.df['bank_name'].value_counts()\n",
    "            # Loop through the results and print them\n",
    "            for bank, count in bank_counts.items():\n",
    "                print(f\"  {bank}: {count}\")\n",
    "\n",
    "            # Print statistics about rating distribution\n",
    "            print(\"\\nRating distribution:\")\n",
    "            # Count occurrences of each rating, and sort by rating (5 down to 1)\n",
    "            rating_counts = self.df['rating'].value_counts().sort_index(ascending=False)\n",
    "            for rating, count in rating_counts.items():\n",
    "                # Calculate percentage for this rating\n",
    "                pct = (count / len(self.df)) * 100\n",
    "                # Print star representation, count, and percentage\n",
    "                print(f\"  {'‚≠ê' * int(rating)}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "            # Print the full date range of the data\n",
    "            print(f\"\\nDate range: {self.df['review_date'].min()} to {self.df['review_date'].max()}\")\n",
    "\n",
    "            # Print statistics about the length of the review texts\n",
    "            print(f\"\\nText statistics:\")\n",
    "            print(f\"  Average length: {self.df['text_length'].mean():.0f} characters\")\n",
    "            print(f\"  Median length: {self.df['text_length'].median():.0f} characters\")\n",
    "            print(f\"  Min length: {self.df['text_length'].min()}\")\n",
    "            print(f\"  Max length: {self.df['text_length'].max()}\")\n",
    "\n",
    "    def process(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STARTING DATA PREPROCESSING\")\n",
    "        print(\"=\" * 60)\n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        self.check_missing_data()\n",
    "        self.handle_missing_values()\n",
    "        self.normalize_dates()\n",
    "        self.clean_text()\n",
    "        self.validate_ratings()\n",
    "        self.prepare_final_output()\n",
    "\n",
    "        # Attempt to save the data. If successful, generate the report.\n",
    "        if self.save_data():\n",
    "            self.generate_report()\n",
    "            return True\n",
    "\n",
    "        # If saving failed, return False\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Create an instance of the ReviewPreprocessor class\n",
    "    preprocessor = ReviewPreprocessor()\n",
    "    # Run the processing pipeline\n",
    "    success = preprocessor.process()\n",
    "\n",
    "    # Check if the process was successful\n",
    "    if success:\n",
    "        print(\"\\n‚úì Preprocessing completed successfully!\")\n",
    "        # Return the processed DataFrame\n",
    "        return preprocessor.df\n",
    "    else:\n",
    "        print(\"\\n‚úó Preprocessing failed!\")\n",
    "        # Return None to indicate failure\n",
    "        return None\n",
    "if __name__ == \"__main__\":\n",
    "    # If run directly, execute the main function\n",
    "    processed_df = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2aa1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4d8916",
   "metadata": {},
   "source": [
    "# All Banks Review and Save in Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google_play_scraper import Sort, reviews_all\n",
    "# Mapping of app packages to bank codes and names\n",
    "bank_mapping = {\n",
    "    \"com.cr2.amolelight\": {\"bank_code\": \"DASHEN\", \"bank_name\": \"Dashen Bank\"},\n",
    "    \"com.combanketh.mobilebanking\": {\"bank_code\": \"CBE\", \"bank_name\": \"Commercial Bank of Ethiopia\"},\n",
    "    \"com.boa.boaMobileBanking\": {\"bank_code\": \"BOA\", \"bank_name\": \"Abyssinia Bank\"},\n",
    "    \"com.sc.awashpay\": {\"bank_code\":\"Awash\",\"bank_name\":\"awash bank\"},\n",
    "    \"com.coopbankoromiasc.OLB\":{\"bank_code\":\"CBO\",\"bank_name\":\"Cooperative bank of oromia\"},\n",
    "    \"com.act.hijira\":{\"bank_code\":\"Hijra\",\"bank_name\":\"Hijra bank\"},\n",
    "    \"com.bunnabanksc.bunnamobile\":{\"bank_code\":\"Buna\", \"bank_name\":\"Bunna bank\"},\n",
    "    \"com.amharabank.Aba_mobile_banking\":{\"bank_code\":\"Amharabank\",\"bank_name\":\"Amhrabank\"},\n",
    "    \"com.oromiabank.mobilebanking\":{\"bank_code\":\"OB\", \"bank_name\":\"Oromia bank\"},\n",
    "    \"com.nib.NibMobileBanking\":{\"bank_code\":\"NIB\", \"bank_name\":\"NIB bank\"}\n",
    "\n",
    "}\n",
    "\n",
    "bank_apps = list(bank_mapping.keys())\n",
    "all_dfs = []\n",
    "\n",
    "def run_scraper(app_package, max_reviews=1000):\n",
    "    print(f\"\\nüîç Scraping reviews for {app_package} ...\")\n",
    "    try:\n",
    "        data = reviews_all(\n",
    "            app_package,\n",
    "            sleep_milliseconds=0,\n",
    "            lang=\"en\",\n",
    "            country=\"et\",  \n",
    "            sort=Sort.NEWEST\n",
    "        )\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"‚ö† No reviews found for {app_package}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df = df[[\"userName\", \"content\", \"score\", \"at\"]]\n",
    "\n",
    "        return df[:max_reviews]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {app_package}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting Scraper for all banks...\")\n",
    "\n",
    "for app in bank_apps:\n",
    "    df = run_scraper(app_package=app, max_reviews=400)\n",
    "\n",
    "    # If scraper returns empty, create an empty DataFrame with expected columns\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"userName\", \"content\", \"score\", \"at\"])\n",
    "\n",
    "    # Map bank info\n",
    "    df[\"bank_code\"] = bank_mapping[app][\"bank_code\"]\n",
    "    df[\"bank_name\"] = bank_mapping[app][\"bank_name\"]\n",
    "\n",
    "    # Rename columns to match desired output\n",
    "    df = df.rename(columns={\n",
    "        \"userName\": \"user_name\",\n",
    "        \"content\": \"review_text\",\n",
    "        \"score\": \"rating\",\n",
    "        \"at\": \"review_date\"\n",
    "    })\n",
    "\n",
    "    # Add missing columns with default values\n",
    "    if \"thumbs_up\" not in df.columns:\n",
    "        df[\"thumbs_up\"] = 0\n",
    "    if \"reply_content\" not in df.columns:\n",
    "        df[\"reply_content\"] = \"\"\n",
    "    df[\"source\"] = \"PlayStore\"\n",
    "\n",
    "    # Add review_id\n",
    "    df = df.reset_index().rename(columns={\"index\": \"review_id\"})\n",
    "    df[\"review_id\"] += 1  # Start IDs from 1\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all bank reviews\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Reorder columns to match CSV format\n",
    "combined_df = combined_df[[\n",
    "    \"review_id\",\n",
    "    \"review_text\",\n",
    "    \"rating\",\n",
    "    \"review_date\",\n",
    "    \"bank_code\",\n",
    "    \"bank_name\",\n",
    "    \"user_name\",\n",
    "    \"thumbs_up\",\n",
    "    \"reply_content\",\n",
    "    \"source\"\n",
    "]]\n",
    "\n",
    "# Ensure output folder exists\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\Project\\Mobile-Banking\\data\\raw\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = os.path.join(output_path, \"reviews_raw.csv\")\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Scraping Finished for all banks.\")\n",
    "print(f\"CSV file saved at: {output_file}\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(combined_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0bb3b2",
   "metadata": {},
   "source": [
    "# Run Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cd34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "from preprocessing import ReviewPreprocessor\n",
    "preprocessor = ReviewPreprocessor()\n",
    "# Run the process\n",
    "success = preprocessor.process()\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Preprocessing finished successfully!\")\n",
    "    df = preprocessor.df\n",
    "else:\n",
    "    print(\"‚ùå Preprocessing failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d918ab2",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 1. Ratings Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='rating', data=df, palette='viridis')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# 2. Reviews per Bank\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='bank_code', data=df, palette='Set2')\n",
    "plt.title('Number of Reviews per Bank')\n",
    "plt.xlabel('Bank')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a268052",
   "metadata": {},
   "source": [
    "# Review Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ee4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='text_length', bins=50, kde=True, hue='bank_code')\n",
    "plt.title('Distribution of Review Lengths by Bank')\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
